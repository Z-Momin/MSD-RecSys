{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit import batch, topn, util\n",
    "from lenskit.algorithms import Recommender, als\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sample(train,test,sample_size):\n",
    "    \n",
    "    user_ids = pd.DataFrame(train.user_id.unique())\n",
    "    \n",
    "    training_ids = user_ids.sample(frac=sample_size, random_state=10)\n",
    "    \n",
    "    training_ids = training_ids.rename(columns={0: \"user_id\"})\n",
    "    \n",
    "    final_train = training_ids.merge(train, on = \"user_id\")\n",
    "    \n",
    "    final_test = training_ids.merge(test, on = \"user_id\")\n",
    "    \n",
    "    return final_train, final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(aname, algo, train, test):\n",
    "    fittable = util.clone(algo)\n",
    "    fittable = Recommender.adapt(fittable)\n",
    "    fittable.fit(train)\n",
    "    users = test.user.unique()\n",
    "    recs = batch.recommend(fittable, users, 500)\n",
    "    recs['Algorithm'] = aname\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('Train_Subsample.parquet')\n",
    "test = pd.read_parquet('Test_Subsample.parquet')\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "nDCG = 0.05990125097411818\n",
      "Time taken for subsample 0.1: 15.020716190338135\n",
      "-----------------------------------------\n",
      "nDCG = 0.08666989196191503\n",
      "Time taken for subsample 0.2: 35.15086579322815\n",
      "-----------------------------------------\n",
      "nDCG = 0.10080064126964208\n",
      "Time taken for subsample 0.3: 53.53020429611206\n",
      "-----------------------------------------\n",
      "nDCG = 0.10904326156800516\n",
      "Time taken for subsample 0.4: 65.25157237052917\n",
      "-----------------------------------------\n",
      "nDCG = 0.11678215431925072\n",
      "Time taken for subsample 0.5: 67.28724408149719\n",
      "-----------------------------------------\n",
      "nDCG = 0.1263111869483915\n",
      "Time taken for subsample 0.6: 43.47783327102661\n",
      "-----------------------------------------\n",
      "nDCG = 0.136684219618948\n",
      "Time taken for subsample 0.7: 80.91423344612122\n",
      "-----------------------------------------\n",
      "nDCG = 0.14081499331877956\n",
      "Time taken for subsample 0.8: 95.11232948303223\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ALS model according to the best parameters from our hyperparameter search\n",
    "als_model = als.ImplicitMF(features=205, iterations=10, reg=0.01, weight=100)\n",
    "\n",
    "sampleSize = [.1*i for i in range(1,9)]\n",
    "\n",
    "for s in sampleSize:\n",
    "    \n",
    "    df_train, df_test = sub_sample(train,test,s)\n",
    "\n",
    "    df_train = df_train.rename(columns={'user_id':'user', 'track_id':'item', 'count':'rating'})\n",
    "\n",
    "    df_test = df_test.rename(columns={'user_id':'user', 'track_id':'item', 'count':'rating'})\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run LensKit model\n",
    "    all_recs = []\n",
    "    test_data = []\n",
    "\n",
    "    test_data.append(df_test)\n",
    "\n",
    "    all_recs.append(eval('ALS', als_model, df_train, df_test))\n",
    "\n",
    "    all_recs = pd.concat(all_recs, ignore_index=True)           \n",
    "    test_data = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "    rla = topn.RecListAnalysis()\n",
    "    rla.add_metric(topn.ndcg)\n",
    "    results = rla.compute(all_recs, test_data)\n",
    "\n",
    "    # Print nDCG and elapsed time for the run\n",
    "    print('-----------------------------------------')\n",
    "    nDCG = results.ndcg.mean()\n",
    "    print('nDCG = {}'.format(nDCG))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print('Time taken for subsample {}: {}'.format(np.round(s,1), elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Precision = 0.0035111111111111124\n",
      "Time taken for subsample 0.1: 25.794238805770874\n",
      "-----------------------------------------\n",
      "Precision = 0.005410526315789478\n",
      "Time taken for subsample 0.2: 37.340272188186646\n",
      "-----------------------------------------\n",
      "Precision = 0.005912408759124091\n",
      "Time taken for subsample 0.3: 47.62299180030823\n",
      "-----------------------------------------\n",
      "Precision = 0.006761904761904767\n",
      "Time taken for subsample 0.4: 59.304797887802124\n",
      "-----------------------------------------\n",
      "Precision = 0.00704347826086957\n",
      "Time taken for subsample 0.5: 67.10406970977783\n",
      "-----------------------------------------\n",
      "Precision = 0.007423357664233579\n",
      "Time taken for subsample 0.6: 77.4145119190216\n",
      "-----------------------------------------\n",
      "Precision = 0.007866242038216543\n",
      "Time taken for subsample 0.7: 84.49690341949463\n",
      "-----------------------------------------\n",
      "Precision = 0.008016483516483491\n",
      "Time taken for subsample 0.8: 97.42567467689514\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ALS model according to the best parameters from our hyperparameter search\n",
    "als_model = als.ImplicitMF(features=205, iterations=10, reg=0.01, weight=100)\n",
    "\n",
    "sampleSize = [.1*i for i in range(1,9)]\n",
    "\n",
    "for s in sampleSize:\n",
    "    \n",
    "    df_train, df_test = sub_sample(train,test,s)\n",
    "\n",
    "    df_train = df_train.rename(columns={'user_id':'user', 'track_id':'item', 'count':'rating'})\n",
    "\n",
    "    df_test = df_test.rename(columns={'user_id':'user', 'track_id':'item', 'count':'rating'})\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run LensKit model\n",
    "    all_recs = []\n",
    "    test_data = []\n",
    "\n",
    "    test_data.append(df_test)\n",
    "\n",
    "    all_recs.append(eval('ALS', als_model, df_train, df_test))\n",
    "\n",
    "    all_recs = pd.concat(all_recs, ignore_index=True)           \n",
    "    test_data = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "    rla = topn.RecListAnalysis()\n",
    "    rla.add_metric(topn.precision)\n",
    "    results = rla.compute(all_recs, test_data)\n",
    "\n",
    "    # Print precision and elapsed time for the run\n",
    "    print('-----------------------------------------')\n",
    "    precision = results.precision.mean()\n",
    "    print('Precision = {}'.format(precision))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print('Time taken for subsample {}: {}'.format(np.round(s,1), elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Precision = 0.0036888888888888896\n",
      "Time taken for subsample 0.1: 22.61983895301819\n",
      "-----------------------------------------\n",
      "Precision = 0.005578947368421057\n",
      "Time taken for subsample 0.2: 35.450098752975464\n",
      "-----------------------------------------\n",
      "Precision = 0.006686131386861318\n",
      "Time taken for subsample 0.3: 45.420161724090576\n",
      "-----------------------------------------\n",
      "Precision = 0.007121693121693127\n",
      "Time taken for subsample 0.4: 55.59146046638489\n",
      "-----------------------------------------\n",
      "Precision = 0.007365217391304352\n",
      "Time taken for subsample 0.5: 64.81741833686829\n",
      "-----------------------------------------\n",
      "Precision = 0.007861313868613131\n",
      "Time taken for subsample 0.6: 73.16239047050476\n",
      "-----------------------------------------\n",
      "Precision = 0.008261146496815263\n",
      "Time taken for subsample 0.7: 82.50107455253601\n",
      "-----------------------------------------\n",
      "Precision = 0.00849999999999997\n",
      "Time taken for subsample 0.8: 93.51557922363281\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ALS model according to the best parameters from our hyperparameter search\n",
    "als_model = als.ImplicitMF(features=205, iterations=10, reg=10, weight=100)\n",
    "\n",
    "sampleSize = [.1*i for i in range(1,9)]\n",
    "\n",
    "for s in sampleSize:\n",
    "    \n",
    "    df_train, df_test = sub_sample(train,test,s)\n",
    "\n",
    "    df_train = df_train.rename(columns={'user_id':'user', 'track_id':'item', 'count':'rating'})\n",
    "\n",
    "    df_test = df_test.rename(columns={'user_id':'user', 'track_id':'item', 'count':'rating'})\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run LensKit model\n",
    "    all_recs = []\n",
    "    test_data = []\n",
    "\n",
    "    test_data.append(df_test)\n",
    "\n",
    "    all_recs.append(eval('ALS', als_model, df_train, df_test))\n",
    "\n",
    "    all_recs = pd.concat(all_recs, ignore_index=True)           \n",
    "    test_data = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "    rla = topn.RecListAnalysis()\n",
    "    rla.add_metric(topn.precision)\n",
    "    results = rla.compute(all_recs, test_data)\n",
    "\n",
    "    # Print precision and elapsed time for the run\n",
    "    print('-----------------------------------------')\n",
    "    precision = results.precision.mean()\n",
    "    print('Precision = {}'.format(precision))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print('Time taken for subsample {}: {}'.format(np.round(s,1), elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
